{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661b37c9",
   "metadata": {},
   "source": [
    "# IMPORTING JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('\"C:\\\\Users\\\\Aditya\\\\Documents\\\\chAt_bot\\\\intents3.json\"') as file:\n",
    "    intents = json.load(file, strict = False) \n",
    "intents = intents['intents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee55a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[\", end = \"\")\n",
    "for intent in intents:\n",
    "  print(\"{\", end = \"\")\n",
    "  for key, value in intent.items():\n",
    "    print(\"{}: {},\".format(key, value))\n",
    "  print(\"\\b\\b\\n},\")\n",
    "print(\"\\b\\b]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d344588",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b66052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tflearn\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('all')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852befa",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ec738",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_model = True\n",
    "\n",
    "if retrain_model:\n",
    "    #This will be a list of all the words used in any of the 'patterns' in each intent\n",
    "    all_words = []\n",
    "    #This will be a list of all the 'tag's associated with the intents\n",
    "    all_tags = [] \n",
    "    #This will be a list containing all of the 'patterns' for each intent where each individual pattern is grouped together\n",
    "    intent_patterns = []\n",
    "    #This will be a list correlated with 'intent_patterns' where every pattern in 'intent_patterns' is correlated with its respective tag in this list\n",
    "    intent_tags = [] \n",
    "    \n",
    "    #Here we fill in all of the lists above. Note that we tokenize the words in each pattern which means we split each pattern into individual words\n",
    "    for intent in intents:\n",
    "        for pattern in intent['patterns']:\n",
    "            words = nltk.word_tokenize(pattern)\n",
    "\n",
    "            all_words.extend(words)\n",
    "            intent_patterns.append(words)\n",
    "            intent_tags.append(intent['tag'])\n",
    "            \n",
    "        all_tags.append(intent['tag'])\n",
    "        \n",
    "    #Here we stem the words in all_words. This means that we reduce every word down to its root form or stem. This will prevent our chatbot from confusin\n",
    "    #Very similar words with eachother. For example, the chatbot might normally confuse the words 'running' and 'run' because they appear different even \n",
    "    #Though they effectively mean the same thing. Stemming will reduce both of these words down to their root form which would be 'run' so the chatbot will\n",
    "    #No longer be confused  \n",
    "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
    "    all_words = sorted(list(set(all_words)))\n",
    "    \n",
    "    all_tags = sorted(all_tags)\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    y_empty = [0 for i in range(len(all_tags))]\n",
    "    \n",
    "   #Here we are creating our training set input and output values for our deep learning algorithm\n",
    "    #We will do this by iterating through our intents and turning each one into a bag of words, or a vector that indicates which words are in each pattern.\n",
    "    #These bags of words will be the x values and the y values will be the intent that each bag of words is associated with.\n",
    "    #The machine learning will train on this data and will be able to determine which bag of words \n",
    "    for index, intent in enumerate(intent_patterns):\n",
    "        bag_of_words = []\n",
    "        \n",
    "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
    "        \n",
    "        for word in all_words:\n",
    "            if word in intent_words:\n",
    "                bag_of_words.append(1)\n",
    "            else:\n",
    "                bag_of_words.append(0)\n",
    "                \n",
    "        one_hot_encode_y = y_empty[:]\n",
    "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
    "        \n",
    "        x_train.append(bag_of_words)\n",
    "        y_train.append(one_hot_encode_y)\n",
    "    \n",
    "    #Here is the data we will be using to train our neural network later\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    #Here we just save our training data so we don't need to process it again if we just want to run our chatbot\n",
    "    with open('training_data.pickle', 'wb') as f:\n",
    "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
    "else:\n",
    "    with open('training_data.pickle', 'rb') as f:\n",
    "        all_words, all_tags, x_train, y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430ad0e",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#Creation the neural network layers\n",
    "neural_net = tflearn.input_data(shape = [None, len(x_train[0])])\n",
    "neural_net = tflearn.fully_connected(neural_net, 8)\n",
    "neural_net = tflearn.fully_connected(neural_net, 8)\n",
    "#Here we use the softmax activation function so the output of our neural network is a probability. We will make use of this later\n",
    "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation = 'softmax')\n",
    "neural_net = tflearn.regression(neural_net)\n",
    "\n",
    "model = tflearn.DNN(neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c4c37",
   "metadata": {},
   "source": [
    "# BATCH GRADIENT DESCEND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee17ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_model:\n",
    "    #Here we train the neural network with the training data we created in the NLP stage\n",
    "    model.fit(x_train, y_train, n_epoch = 1000, batch_size = 8, show_metric = True)\n",
    "    model.save('model.tfl')\n",
    "else:\n",
    "    model.load('./model.tfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bag(text, all_words):\n",
    "    #Initialize the bag of words by creating an empty slot for every word in the vector\n",
    "    bag_of_words = [0 for i in range(len(all_words))]\n",
    "    \n",
    "    #First we split up the input into individual words and stem them so they match the same format as in our vector\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
    "    \n",
    "    #Now we create the bag of words by filling in a 1 for the words that the user used\n",
    "    for word in text_words:\n",
    "        if word in all_words:\n",
    "            bag_of_words[all_words.index(word)] = 1\n",
    "    \n",
    "    #And return the bag of words\n",
    "    return np.array(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44589f0",
   "metadata": {},
   "source": [
    "# SPEECH RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()\n",
    "import pyttsx3\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93065ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_speaking(message):\n",
    "    engine.say(message)\n",
    "    engine.runAndWait()\n",
    "    if engine._inLoop:\n",
    "        engine.endLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed03311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    with sr.Microphone() as source:\n",
    "        #print(\"Say something!!!\")\n",
    "        bot_speaking(\"Hey mate say something!\")\n",
    "        audio = r.listen(source,timeout=0)\n",
    "        bot_speaking(\"Perfect, Thanks!\")\n",
    "    try:\n",
    "        msg = r.recognize_google(audio)\n",
    "        print(\"TEXT: \"+msg);\n",
    "        bot_speaking(\"you said \"+msg)\n",
    "        return msg\n",
    "    except:\n",
    "        bot_speaking(\"Sorry mate! It's not working\")\n",
    "        pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd338f5",
   "metadata": {},
   "source": [
    "# CHAT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    #Starting message\n",
    "    print(\"BOT : Hi! I am your assistance bot. I am here to answer your queries\")\n",
    "    \n",
    "    #Reset the context state since there is no context at the beginning of the conversation\n",
    "    context_state = None\n",
    "    \n",
    "    #This is what the bot will say if it doesn't understand what the user is saying\n",
    "    default_responses = ['Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!',\n",
    "                         'You confuse me human. Lets talk about something else.',\n",
    "                         'Im not sure what that means and I dont really care. Lets talk about something else',\n",
    "                         'I dont understand that! Try rephrasing or saying something else.']\n",
    "    \n",
    "    #This chat loop will go on forever until the user types quit\n",
    "    while True:\n",
    "        user_chat = str(input('YOU : '))\n",
    "        if user_chat.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        #Convert chat to bag of words\n",
    "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
    "        \n",
    "        #Pass bag of words into our neural network\n",
    "        response = model.predict([user_chat_bag])[0]\n",
    "        \n",
    "        #Get the intent that the bag of words is most highly correlated with\n",
    "        response_index = np.argmax(response)\n",
    "        response_tag = all_tags[response_index]\n",
    "        \n",
    "        #If the neural network is fairly certain that it has chosen the right intent (and isnt randomly guessing)\n",
    "        #In this case, we will only get a response if the neural network is more than 80% certain\n",
    "        if response[response_index] > 0.8:\n",
    "            for intent in intents:\n",
    "                #Get the intent that is predicted\n",
    "                if intent['tag'] == response_tag:\n",
    "                    #Check if this response is associated with a specific context\n",
    "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
    "                        #Get all of the possible responses from this intent\n",
    "                        possible_responses = intent['responses']\n",
    "                        #If this intent is associated with a context set, then set the context state\n",
    "                        if 'context_set' in intent:\n",
    "                            context_state = intent['context_set']\n",
    "                        else:\n",
    "                            context_state = None\n",
    "                        #Select a random message from the intent responses\n",
    "                        ms = random.choice(possible_responses)\n",
    "                        print(\"BOT : \"+ms)\n",
    "                        bot_speaking(ms)\n",
    "                    else:\n",
    "                        ms = random.choice(default_responses)\n",
    "                        print(\"BOT : \"+ms)\n",
    "                        bot_speaking(ms)\n",
    "        else:\n",
    "            ms = random.choice(default_responses)\n",
    "            print(\"BOT : \"+ms)\n",
    "            bot_speaking(ms)\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
